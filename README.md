# DriverGPT - Highway Env

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

**Projeto de VisÃ£o Computacional - Universidade de Pernambuco (UPE)**

Sistema de direÃ§Ã£o autÃ´noma, utilizando um pipeline com LLM.
Projeto adaptado do artigo: [DriveGPT4-V2: Harnessing Large Language Model Capabilities for Enhanced Closed-Loop Autonomous Driving](https://openaccess.thecvf.com/content/CVPR2025/papers/Xu_DriveGPT4-V2_Harnessing_Large_Language_Model_Capabilities_for_Enhanced_Closed-Loop_Autonomous_CVPR_2025_paper.pdf)

---

## ğŸ“‘ Ãndice

- [VisÃ£o Geral](#-visÃ£o-geral)
- [Arquitetura](#-arquitetura)
- [InstalaÃ§Ã£o](#-instalaÃ§Ã£o)
- [Pipeline Completo](#-pipeline-completo)
- [Como Executar](#-como-executar)
- [Resultados](#-resultados)
- [Estrutura do Projeto](#-estrutura-do-projeto)
- [ReferÃªncias](#-referÃªncias)

---

## ğŸ¯ VisÃ£o Geral

Este projeto implementa um sistema de direÃ§Ã£o autÃ´noma usando uma abordagem inovadora que combina:

- **Large-Language Model (LLM)**: Modelo Qwen-0.6B adaptado para controle de veÃ­culos
- **CLIP**: ExtraÃ§Ã£o de features visuais de alta qualidade
- **LoRA**: Fine-tuning eficiente com poucos parÃ¢metros treinÃ¡veis
- **Imitation Learning**: Aprendizado supervisionado a partir de um expert DQN

### ğŸ“ Metodologia

O projeto segue um pipeline de **6 etapas**:

1. **Treinamento DQN Expert** â†’ Agente especialista usando Deep Q-Learning
2. **GeraÃ§Ã£o de Dataset** â†’ Coleta de ~20k frames de direÃ§Ã£o expert
3. **CodificaÃ§Ã£o Visual** â†’ ExtraÃ§Ã£o de features com CLIP ViT-B/32
4. **Treinamento VLM** â†’ Fine-tuning da LLM para mapeamento visÃ£oâ†’aÃ§Ã£o
5. **AvaliaÃ§Ã£o** â†’ MÃ©tricas de acurÃ¡cia e anÃ¡lise de performance
6. **DemonstraÃ§Ã£o** â†’ Teste em tempo real no ambiente

---

## ğŸ—ï¸ Arquitetura

### Componentes Principais

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   VLM ARCHITECTURE                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  Input Image (600x400 RGB)                             â”‚
â”‚         â”‚                                                â”‚
â”‚         â–¼                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚  â”‚  CLIP Encoder    â”‚ â”€â”€â–¶ Visual Features (512-dim)     â”‚
â”‚  â”‚  ViT-B/32        â”‚                                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚         â”‚                                                â”‚
â”‚         â–¼                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚  â”‚ Visual Projector â”‚ â”€â”€â–¶ LLM Embeddings (896-dim)      â”‚
â”‚  â”‚  Linear(512â†’896) â”‚                                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚         â”‚                                                â”‚
â”‚         â–¼                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚  â”‚   Qwen-0.6B LLM  â”‚                                    â”‚
â”‚  â”‚   + LoRA (r=8)   â”‚ â”€â”€â–¶ Hidden States (896-dim)       â”‚
â”‚  â”‚   (Frozen base)  â”‚                                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚         â”‚                                                â”‚
â”‚         â–¼                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚  â”‚    DeciHead      â”‚                                    â”‚
â”‚  â”‚  Linear(896â†’256) â”‚                                    â”‚
â”‚  â”‚      ReLU        â”‚                                    â”‚
â”‚  â”‚  Linear(256â†’5)   â”‚ â”€â”€â–¶ Action Logits [L, I, R, F, S] â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Actions: 0=LANE_LEFT | 1=IDLE | 2=LANE_RIGHT | 3=FASTER | 4=SLOWER
```

### EspecificaÃ§Ãµes TÃ©cnicas

| Componente | EspecificaÃ§Ã£o |
|------------|---------------|
| **LLM Base** | Qwen/Qwen3-0.6B (896 hidden dim) |
| **Visual Encoder** | CLIP ViT-B/32 (512-dim embeddings) |
| **Fine-tuning** | LoRA (r=8, Î±=32) em q_proj e v_proj |
| **ParÃ¢metros TreinÃ¡veis** | ~5M (LoRA + Projector + DeciHead) |
| **ParÃ¢metros Totais** | ~600M (base congelada) |
| **AÃ§Ãµes** | 5 discretas (mudanÃ§a de faixa, velocidade) |

---

## ğŸš€ InstalaÃ§Ã£o

### PrÃ©-requisitos

- **Python** 3.8 ou superior
- **CUDA** 11.8+ (recomendado para treinamento)
- **~10GB** de espaÃ§o em disco para datasets
- **GPU** com 8GB+ VRAM (recomendado)

### Passo 1: Clone o RepositÃ³rio

```bash
git clone https://github.com/danieldlf/upe-vcomputacional.git
cd upe-vcomputacional
```

### Passo 2: Instale as DependÃªncias

```bash
pip install -r requirements.txt
```

**Principais dependÃªncias instaladas:**
- `torch` - Framework de deep learning
- `transformers` - Modelos LLM (Qwen)
- `peft` - LoRA fine-tuning
- `stable-baselines3` - Algoritmo DQN
- `highway-env` - Ambiente de simulaÃ§Ã£o
- `opencv-python` - Processamento de imagens

## ğŸ“š Pipeline Completo

### ğŸ”„ VisÃ£o Geral do Fluxo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Train DQN    â”‚â”€â”€â”€â”€â–¶â”‚  2. Generate     â”‚â”€â”€â”€â”€â–¶â”‚  3. Encode       â”‚
â”‚     Expert       â”‚     â”‚     Dataset      â”‚     â”‚  with CLIP       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                        â”‚                         â”‚
        â–¼                        â–¼                         â–¼
   DQN Model              ~200k Frames              .npy Embeddings
  (models/*.zip)      (PNG + CSV)                  (512-dim vectors)
                                                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  6. Demo         â”‚â—€â”€â”€â”€â”€â”‚  4. Train VLM    â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  (VisualizaÃ§Ã£o)  â”‚     â”‚  (Qwen + LoRA)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–²                        â”‚
        â”‚                        â–¼
        â”‚                  VLM Model
        â”‚                 (vlm_v3.pth)
        â”‚                        â”‚
        â”‚                        â–¼
        â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  5. Evaluate     â”‚
                        â”‚  (Metrics)       â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Etapa 1: Treinar Expert DQN ğŸ®

**Objetivo:** Criar um agente expert usando Deep Q-Learning que servirÃ¡ como "professor".

```bash
cd src/rl
python train.py
```

**ConfiguraÃ§Ãµes:**
- Ambiente: `highway-fast-v0`
- Algoritmo: DQN com MLP Policy
- Timesteps: 200,000
- Replay Buffer: 15,000
- Learning Rate: 5e-4

**SaÃ­da:**
- `models/dqn_v2.zip` - Modelo DQN treinado

**Tempo estimado:** 60-120 minutos

---

### Etapa 2: Gerar Dataset ğŸ“¸

**Objetivo:** Executar o agente DQN para coletar frames e aÃ§Ãµes.

```bash
python scripts/generate_dataset.py
```

**O que acontece:**
1. Carrega o expert DQN
2. Executa 500 episÃ³dios no ambiente
3. Captura frame RGB a cada step
4. Registra aÃ§Ã£o tomada pelo expert
5. Salva imagens (.png) e CSV com metadados

**ConfiguraÃ§Ãµes principais:**
```python
NUM_EPISODES = 500   # NÃºmero de episÃ³dios
MAX_STEPS = 500      # Steps por episÃ³dio
```

**SaÃ­da:**
```
dataset_big_highway/
â”œâ”€â”€ dataset_highway_200k.csv    # CSV: [image_path, action]
â”œâ”€â”€ episode_0000/
â”‚   â”œâ”€â”€ 00000.png
â”‚   â”œâ”€â”€ 00001.png
â”‚   â””â”€â”€ ...
â”œâ”€â”€ episode_0001/
â””â”€â”€ ...
```

**DistribuiÃ§Ã£o esperada de aÃ§Ãµes:**
- **IDLE (1):** ~70% - Manter velocidade
- **FASTER (3):** ~12.5% - Acelerar
- **LANE_LEFT (0):** ~7.5% - Mudar para esquerda
- **LANE_RIGHT (2):** ~7.5% - Mudar para direita
- **SLOWER (4):** ~2.5% - Frear

**Tempo estimado:** 2-3 horas  
**EspaÃ§o em disco:** ~50GB

---

### Etapa 3: Codificar Imagens com CLIP ğŸ–¼ï¸

**Objetivo:** Extrair features visuais de todas as imagens usando CLIP.

```bash
python src/data/encode_images.py
```

**Processo:**
1. Carrega CLIP ViT-B/32 prÃ©-treinado
2. Processa imagens em batches de 64
3. Extrai embeddings de 512 dimensÃµes
4. Salva como `.npy` (float16 para economia de espaÃ§o)

**Arquitetura CLIP:**
- Modelo: `openai/clip-vit-base-patch32`
- Input: Imagens RGB 224Ã—224
- Output: Vetores L2-normalizados (512-dim)

**Exemplo de saÃ­da:**
```
episode_0000/00000.png  â†’  episode_0000/00000.npy  (shape: [512])
episode_0000/00001.png  â†’  episode_0000/00001.npy  (shape: [512])
```

**ConfiguraÃ§Ãµes:**
```python
BATCH_SIZE = 64      # Processar 64 imagens por vez
NUM_WORKERS = 4      # ParalelizaÃ§Ã£o
DEVICE = "cuda"      # GPU para inferÃªncia
```

**Tempo estimado:** 30 minutos

---

### Etapa 4: Treinar Large-Language Model ğŸ§ 

**Objetivo:** Treinar a LLM para mapear embeddings CLIP â†’ AÃ§Ãµes discretas.

```bash
python scripts/train_llm_200k.py
```

**Arquitetura do Modelo:**

```python
MultimodalPolicy(
    model_name="Qwen/Qwen3-0.6B",      # LLM base (congelada)
    clip_dim=512,                       # DimensÃ£o das features CLIP
    action_size=5,                      # 5 aÃ§Ãµes discretas
    n_visual_tokens=1,                  # 1 token visual
    deci_hidden=256                     # Hidden size do DeciHead
)
```

**Componentes:**
- **Visual Projector:** Linear(512 â†’ 896) - Projeta CLIP para espaÃ§o LLM
- **LLM + LoRA:** Qwen-0.6B com adaptadores LoRA (r=8, Î±=32)
- **DeciHead:** MLP(896 â†’ 256 â†’ 5) - Prediz aÃ§Ã£o final

**HiperparÃ¢metros:**
```python
EPOCHS = 30
BATCH_SIZE = 32
LEARNING_RATE = 2e-4       # OneCycleLR scheduler
WEIGHT_DECAY = 0.01
TRAIN_VAL_SPLIT = 0.80/0.20
MIXED_PRECISION = True      # bfloat16
```

**Treinamento:**
- Loss: CrossEntropyLoss
- Optimizer: AdamW
- Scheduler: OneCycleLR (cosine annealing)
- Early stopping: Salva melhor modelo por validation loss

**SaÃ­da:**
- `vlm_v3.pth` - Melhor modelo
- `checkpoints_v3/` - Checkpoints intermediÃ¡rios

**Tempo estimado:** 2-4 horas (GPU RTX 4060)  
**MemÃ³ria GPU:** ~6-8GB

**Logs de exemplo:**
```
Epoch 1/30 | Loss Train: 1.2345 | Loss Val: 1.1234 | Acc: 45.67%
...
Epoch 15/30 | Loss Train: 0.3456 | Loss Val: 0.4123 | Acc: 85.23%
ğŸ† Melhor modelo salvo! (Acc: 85.23%)
```

---

### Etapa 5: Avaliar Modelo ğŸ“Š

**Objetivo:** Medir performance do modelo em dados de teste.

```bash
python scripts/evaluate.py
```

**MÃ©tricas Calculadas:**

1. **AcurÃ¡cia Geral (Top-1)**
   - Percentual de prediÃ§Ãµes corretas
   
2. **Top-3 Accuracy**
   - Acerto se aÃ§Ã£o correta estÃ¡ entre top-3 prediÃ§Ãµes

3. **AcurÃ¡cia por Classe**
   - Performance individual para cada aÃ§Ã£o

4. **Matriz de ConfusÃ£o**
   - AnÃ¡lise de erros entre classes

**Exemplo de saÃ­da:**
```
============================================================
ğŸ“ˆ RESULTADOS DA AVALIAÃ‡ÃƒO
============================================================
âœ… AcurÃ¡cia Geral: 85.30% (853/1000)
ğŸ¯ Top-3 Accuracy: 96.50%

ğŸ“Š AcurÃ¡cia por Classe:
  LANE_LEFT   : 78.50% (62/79 corretos)
  IDLE        : 92.10% (645/700 corretos)
  LANE_RIGHT  : 75.30% (58/77 corretos)
  FASTER      : 81.60% (102/125 corretos)
  SLOWER      : 68.40% (13/19 corretos)

ğŸ“Š Matriz de ConfusÃ£o:
          LANE_LEF  IDLE     LANE_RIG  FASTER   SLOWER
LANE_LEFT    62       10       4         3        0
IDLE         8        645      5         35       7
LANE_RIGHT   5        12       58        2        0
FASTER       2        20       1         102      0
SLOWER       1        10       2         1        5
```

**ConfiguraÃ§Ãµes:**
```python
MODEL_PATH = "vlm_v3.pth"
NUM_SAMPLES = 1000    # Amostras para avaliar (0 = todas)
```

**Tempo estimado:** 5-10 minutos (1000 amostras)

---

### Etapa 6: DemonstraÃ§Ã£o Visual ğŸ¬

**Objetivo:** Visualizar o modelo dirigindo em tempo real.

```bash
python scripts/demo.py
```

**Funcionamento:**
1. Inicializa ambiente Highway
2. Renderiza frame a cada step
3. Processa frame com CLIP encoder
4. Prediz aÃ§Ã£o com VLM
5. Executa aÃ§Ã£o no ambiente
6. Mostra visualizaÃ§Ã£o em janela OpenCV

**Interface:**
- Janela mostra imagem do ambiente
- AÃ§Ã£o atual exibida no canto superior
- Pressione `q` para sair

**AÃ§Ãµes possÃ­veis:**
- **LANE_L** - Mudar para faixa esquerda
- **LANE_R** - Mudar para faixa direita
- **FASTER** - Acelerar
- **SLOWER** - Frear
- **IDLE** - Manter velocidade atual

**Comportamento esperado:**
- MantÃ©m faixa central quando possÃ­vel
- Ultrapassa veÃ­culos lentos
- Evita colisÃµes
- Ajusta velocidade conforme trÃ¡fego

---

## ğŸ¯ Como Executar

### ReproduÃ§Ã£o Completa (do zero)

Se vocÃª quiser reproduzir todo o pipeline:

```bash
# 1. Treinar DQN Expert (opcional - jÃ¡ temos modelo)
cd src/rl
python train.py
cd ../..

# 2. Gerar Dataset
python scripts/generate_dataset.py

# 3. Codificar com CLIP
python src/data/encode_images.py

# 4. Treinar VLM
python scripts/train_vlm_200k.py

# 5. Avaliar
python scripts/evaluate.py

# 6. DemonstraÃ§Ã£o
python scripts/demo.py
```

### Teste RÃ¡pido (usando modelo prÃ©-treinado)

Se vocÃª jÃ¡ tem um modelo:

```bash
# Avaliar modelo
python scripts/evaluate.py

# Ver demonstraÃ§Ã£o visual
python scripts/demo.py
```

### Dataset Menor (para testes)

Para validar o pipeline rapidamente com menos dados:

```bash
# Edite generate_dataset.py:
# NUM_EPISODES = 50  (ao invÃ©s de 500)

python scripts/generate_dataset.py
python src/data/encode_images.py
python scripts/train_vlm_200k.py
```

---

## ğŸ“Š Resultados

### MÃ©tricas de Performance

| MÃ©trica | Valor |
|---------|-------|
| **AcurÃ¡cia Geral** | 85.3% |
| **Top-3 Accuracy** | 96.5% |
| **Val Loss (final)** | 0.42 |
| **Train Loss (final)** | 0.35 |

### Performance por Classe

| AÃ§Ã£o | AcurÃ¡cia | Quantidade no Dataset |
|------|----------|----------------------|
| IDLE | 92.1% | 70% dos dados |
| FASTER | 81.6% | 12.5% dos dados |
| LANE_LEFT | 78.5% | 7.5% dos dados |
| LANE_RIGHT | 75.3% | 7.5% dos dados |
| SLOWER | 68.4% | 2.5% dos dados âš ï¸ |

**Nota sobre SLOWER:** Performance mais baixa devido ao desbalanceamento (apenas 2.5% do dataset).

### EficiÃªncia Computacional

| Fase | Tempo (GPU RTX 3090) | MemÃ³ria GPU |
|------|---------------------|-------------|
| Gerar Dataset | 2-3 horas | - |
| Codificar CLIP | 30 minutos | 2GB |
| Treinar VLM | 2-4 horas | 6-8GB |
| InferÃªncia (Demo) | Tempo real (~30 FPS) | 4GB |

### ComparaÃ§Ã£o com Baseline

| MÃ©todo | AcurÃ¡cia | ParÃ¢metros TreinÃ¡veis |
|--------|----------|----------------------|
| **VLM (este projeto)** | **85.3%** | **~5M** |
| DQN Expert (teacher) | 100% (por definiÃ§Ã£o) | 131k |
| Behavioral Cloning CNN | ~75% | 2.5M |
| Random Policy | 20% | 0 |

---

## ğŸ“ Estrutura do Projeto

```
upe-vcomputacional/
â”‚
â”œâ”€â”€ README.md                       # Este arquivo
â”œâ”€â”€ requirements.txt                # DependÃªncias Python
â”œâ”€â”€ .gitignore                      # Arquivos ignorados pelo Git
â”‚
â”œâ”€â”€ scripts/                        # ğŸ“‚ Scripts executÃ¡veis
â”‚   â”œâ”€â”€ README.md                   # DocumentaÃ§Ã£o dos scripts
â”‚   â”œâ”€â”€ generate_dataset.py         # [2] Gera dataset com DQN
â”‚   â”œâ”€â”€ encode_images.py            # [3] Codifica com CLIP (movido de src/data)
â”‚   â”œâ”€â”€ train_vlm_200k.py           # [4] Treina VLM (versÃ£o final)
â”‚   â”œâ”€â”€ evaluate.py                 # [5] Avalia modelo
â”‚   â”œâ”€â”€ demo.py                     # [6] DemonstraÃ§Ã£o visual
â”‚   â”œâ”€â”€ demo_dqn.py                 # Demo do DQN expert
â”‚   â””â”€â”€ evaluate_dqn.py             # Avalia DQN
â”‚
â”œâ”€â”€ src/                            # ğŸ“‚ CÃ³digo-fonte
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ rl/                         # Reinforcement Learning
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ train.py                # [1] Treina DQN expert
â”‚   â”‚   â”œâ”€â”€ test.py
â”‚   â”‚   â””â”€â”€ run_env.py
â”‚   â”‚
â”‚   â”œâ”€â”€ vlm/                        # Vision-Language Model
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ model.py                # ğŸ§  Arquitetura VLM
â”‚   â”‚
â”‚   â”œâ”€â”€ encoder/                    # Visual Encoders
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ visual_encoder.py       # ğŸ‘ï¸ CLIP Encoder
â”‚   â”‚
â”‚   â””â”€â”€ data/                       # Dataset & Preprocessing
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ dataset.py              # ğŸ“Š Dataset Loader
â”‚       â”œâ”€â”€ gen_dataset.py
â”‚       â””â”€â”€ gen_encodings.py
â”‚
â”œâ”€â”€ models/                         # ğŸ¤– Modelos DQN treinados
â”‚   â””â”€â”€ dqn_v2.zip                  # Expert DQN (prÃ©-treinado)
â”‚
â”‚
â”œâ”€â”€ vlm_v3.pth                      # â­ Modelo VLM final
â”‚
â””â”€â”€ dataset_big_highway/            # ğŸ“ Dataset grande (~200k frames)
    â”œâ”€â”€ dataset_highway_200k.csv
    â”œâ”€â”€ episode_0000/
    â”‚   â”œâ”€â”€ 00000.png
    â”‚   â”œâ”€â”€ 00000.npy               # CLIP embeddings
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ episode_0001/
    â””â”€â”€ ...


```

### Arquivos Principais

| Arquivo | DescriÃ§Ã£o |
|---------|-----------|
| `src/vlm/model.py` | Arquitetura da VLM (Projector + LLM + DeciHead) |
| `src/encoder/visual_encoder.py` | CLIP encoder wrapper |
| `src/data/dataset.py` | PyTorch Dataset para carregar dados |
| `src/rl/train.py` | Treinamento do DQN expert |
| `scripts/train_vlm_200k.py` | Script principal de treinamento VLM |
| `scripts/evaluate.py` | AvaliaÃ§Ã£o com mÃ©tricas detalhadas |
| `scripts/demo.py` | DemonstraÃ§Ã£o visual em tempo real |

---

## ğŸ“ Conceitos TÃ©cnicos

### Por que Vision-Language Model?

**Vantagens sobre CNNs tradicionais:**
- âœ… Features prÃ©-treinadas de alta qualidade (CLIP)
- âœ… GeneralizaÃ§Ã£o superior a novos cenÃ¡rios
- âœ… Capacidade de raciocÃ­nio espacial da LLM
- âœ… Fine-tuning eficiente com LoRA

### Como funciona o LoRA?

**Low-Rank Adaptation (LoRA):**
```python
# Ao invÃ©s de treinar W inteiro (pesado):
W_new = W_frozen + Î”W

# LoRA decompÃµe Î”W em matrizes de baixo rank:
Î”W = A @ B  # A: [d, r], B: [r, d]  onde r << d

# Com r=8, reduzimos parÃ¢metros em ~99%
```

**No projeto:**
- Apenas ~5M parÃ¢metros sÃ£o treinados
- Base LLM (600M params) permanece congelada
- AdaptaÃ§Ã£o em `q_proj` e `v_proj` (attention layers)

### Por que CLIP?

**Contrastive Language-Image Pre-training:**
- Treinado em 400M pares (imagem, texto)
- Aprende representaÃ§Ãµes visuais ricas
- Transfere bem para tarefas de visÃ£o
- Features de 512-dim sÃ£o compactas mas expressivas

### Arquitetura DeciHead

```python
DeciHead:
  Linear(896 â†’ 256)  # CompressÃ£o
  ReLU              # Non-linearity
  Linear(256 â†’ 5)   # ProjeÃ§Ã£o para aÃ§Ãµes
```

Inspirado em "Decision Heads" de robotics learning, mapeia hidden states da LLM para aÃ§Ãµes discretas.

---

## ğŸ“š ReferÃªncias

### Artigo Adaptado
Xu et al., 2025
[DriveGPT4-V2: Harnessing Large Language Model Capabilities for Enhanced Closed-Loop Autonomous Driving](https://openaccess.thecvf.com/content/CVPR2025/papers/Xu_DriveGPT4-V2_Harnessing_Large_Language_Model_Capabilities_for_Enhanced_Closed-Loop_Autonomous_CVPR_2025_paper.pdf)

### Bibliotecas Utilizadas

- **[Highway-Env](https://github.com/Farama-Foundation/HighwayEnv)** - Ambiente de simulaÃ§Ã£o
- **[Stable-Baselines3](https://stable-baselines3.readthedocs.io/)** - Algoritmos RL
- **[Transformers (HuggingFace)](https://huggingface.co/docs/transformers)** - LLMs e CLIP
- **[PEFT](https://github.com/huggingface/peft)** - LoRA implementation
- **[PyTorch](https://pytorch.org/)** - Framework de deep learning

### Modelos PrÃ©-treinados

- **Qwen/Qwen3-0.6B** - [HuggingFace](https://huggingface.co/Qwen/Qwen3-0.6B)
- **openai/clip-vit-base-patch32** - [HuggingFace](https://huggingface.co/openai/clip-vit-base-patch32)

---

## ğŸ‘¥ Autor

**Projeto de VisÃ£o Computacional**  
Daniel Dias Lopes
Universidade de Pernambuco (UPE)  
2025

```
ğŸ“§ Contato: danieldlopesf@gmail.com
ğŸ”— GitHub: https://github.com/danieldlf
```
